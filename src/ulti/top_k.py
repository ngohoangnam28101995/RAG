import numpy as np
from src.ulti.get_embedding import get_embedding
from src.ulti.response import response
from src.ulti.hyde import hyde
from src.ulti.mmr import apply_mmr

def top_k(query_text, db_collection, model, tokenizer, device, use_hyde=True, use_reader=False, k=5, use_mmr=True, lambda_param=0.5):
    if use_hyde:
        new_query_text = hyde(query_text)
        query_embedding = get_embedding(new_query_text.lower(), model, tokenizer, device)
        print("Hyde prompt:", new_query_text)
    else:
        query_embedding = get_embedding(query_text.lower(), model, tokenizer, device)

    # Truy váº¥n cÃ¡c vÄƒn báº£n tÆ°Æ¡ng tá»±
    results = db_collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=k * 2 if use_mmr else k,  # Láº¥y nhiá»u hÆ¡n Ä‘á»ƒ MMR cÃ³ thá»ƒ lá»c
        include=['embeddings', 'documents', 'metadatas', 'distances']
    )
    # Náº¿u dÃ¹ng MMR, lá»c káº¿t quáº£ Ä‘á»ƒ tÄƒng Ä‘á»™ Ä‘a dáº¡ng
    if use_mmr and results["embeddings"] is not None:
        print(results["embeddings"][0])
        doc_embeddings = np.array(results["embeddings"][0])  # DÃ¹ng embeddings thay vÃ¬ documents
        mmr_indices = apply_mmr(query_embedding, doc_embeddings, top_k=k, lambda_param=lambda_param)

        results["metadatas"][0] = [results["metadatas"][0][i] for i in mmr_indices]


    retrieved_chunk = ""
    for i, meta in enumerate(results["metadatas"][0]):
        if retrieved_chunk == "":
            #retrieved_chunk = f"ThÃ´ng tin {i+1}: {meta['content']} (Tá»‡p: {meta['filename']})"
            retrieved_chunk = f"ThÃ´ng tÆ° : {meta['filename']} {meta['content']}"
        else:
            #retrieved_chunk += "\n" + f"ThÃ´ng tin {i+1}: {meta['content']} (Tá»‡p: {meta['filename']})"
            retrieved_chunk += "\n" + f"ThÃ´ng tÆ° : {meta['filename']} : {meta['content']}"
        #print(f"ğŸ” ThÃ´ng tin {i+1}: {meta['content']} (Tá»‡p: {meta['filename']})")
        print(f"ThÃ´ng tÆ° : {meta['filename']} : {meta['content']}")

    if use_reader:
        reader_prompt = f'''Báº¡n lÃ  má»™t chuyÃªn gia trong vá» lÄ©nh vá»±c tÃ i chÃ­nh. 
                        DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘oáº¡n thÃ´ng tin liÃªn quan Ä‘Æ°á»£c truy xuáº¥t:
                        ---------------------
                        {retrieved_chunk}
                        ---------------------
                        
                        Vá»›i nhá»¯ng thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p trong "CÃ¡c Ä‘oáº¡n thÃ´ng tin liÃªn quan", Ä‘Ã¢y lÃ  nhá»¯ng thÃ´ng tin Ä‘Æ°á»£c truy váº¥n vá»›i cÃ¢u há»i {query_text}.
                        HÃ£y tá»•ng há»£p cÃ¡c thÃ´ng tin quan trá»ng má»™t cÃ¡ch dá»… hiá»ƒu, sÃºc tÃ­ch, Ä‘áº§y Ä‘á»§ ná»™i dung chÃ­nh vÃ  Ä‘áº£m báº£o chÃºng giáº£i Ä‘Ã¡p Ä‘Æ°á»£c cÃ¢u há»i {query_text}.
                        KhÃ´ng sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i. Má»—i cÃ¢u tráº£ lá»i cáº§n kÃ¨m theo nguá»“n trÃ­ch dáº«n chÃ­nh xÃ¡c theo Ä‘á»‹nh dáº¡ng [Nguá»“n: ThÃ´ng tÆ° sá»‘ ...].
                        **CÃ¢u há»i:** {query_text}  
                        **Káº¿t quáº£ tÃ³m táº¯t:**'''
        
        retrieved_chunk, _ = response(reader_prompt, reader_prompt, temperature=0.8, max_token=4096, past_messages=None, model="vistral-7b-chat", API_URL="http://localhost:1234/v1/chat/completions")

    rag = f''' Báº¡n lÃ  má»™t chuyÃªn gia giáº£i Ä‘Ã¡p tháº¯c máº¯c. 
            ThÃ´ng tin ngá»¯ cáº£nh:  
            --------------------   
            {retrieved_chunk}  
            -------------------- 
            
            Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong "ThÃ´ng tin ngá»¯ cáº£nh" Ä‘á»ƒ tráº£ lá»i: {query_text}. 
            KhÃ´ng sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i. Äáº£m báº£o cÃ¢u tráº£ lá»i cÃ³ nguá»“n trÃ­ch dáº«n chÃ­nh xÃ¡c theo Ä‘á»‹nh dáº¡ng [Nguá»“n: ThÃ´ng tÆ° sá»‘ ...].
            **Truy váº¥n:** {query_text}  
            **CÃ¢u tráº£ lá»i:**'''
    
    return rag, query_text